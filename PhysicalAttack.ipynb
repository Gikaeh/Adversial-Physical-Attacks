{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Tutorial for Physical Attack on a white box model\n",
    "\n",
    "___\n",
    "## Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Prequisites](#Prerequisites)\n",
    "2. [Physical Attack Class](#Physical-Attack-Class)\n",
    "3. [GUI Setup](#gui-build-with-ipywidget)\n",
    "4. [Run Attack w/ GUI](#Run-attack-with-GUI)\n",
    "5. [Test w/ Different Masks on STL10](#Test-different-masks-with-STL10-test-images)\n",
    "6. [Real Physical Attack Examples](#Physical-attacks-on-real-world-examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Why? \n",
    "The disturbance has been produced without being restricted to a specific picture region using the techniques that have been demonstrated thus far in class. Furthermore, the other techniques produced the disturbance on the pictures without considering the printability of the colors. This will enable users to employ the perturbation in the real world by considering the printability of colors. Along with restricting the perturbation to specific regions of the image to further avoid recognition.\n",
    "\n",
    "### How does it work in general?\n",
    "It is easier than the FGSM, BIM, and PGD methods since we don't have to figure out the gradient of the input pixels. Our perturbation, $\\delta$, will be defined as a variable whose values are optimized. In order to modify the components in $\\delta$, the optimizer will first multiply some parts of $\\delta$ by a mask. As a result, the disturbance only impacts particular regions of the picture.\n",
    "\n",
    "Loss 1: The loss 1 includes how well the model is at predicting our desired target class.\n",
    "- In a targeted attack, the \"ground truth\" label, that is passed to the loss function, is the desired target. If the loss is small, the model will predict the target class:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}_1 = crossentropy(\\theta, x, T)\n",
    "\\end{equation*}\n",
    "\n",
    "- In an untargeted attack, the true ground label is passed to the loss function. Therefore, a small loss indicates that the model makes the correct prediction. Since we want the model to predict anything but the true class, you need compute the inverse of the crossentropy value. To avoid division by zero, add a small number $\\gamma$ to the denominator: \n",
    "\\begin{equation*}\n",
    "\\mathcal{L}_1 = \\frac{1}{crossentropy(\\theta, x, y) + \\gamma}\n",
    "\\end{equation*}\n",
    "\n",
    "Loss 2: The loss 2 is influenced by an error that is called printer error. The printer error measures how close each pixel value is to printable colors. Loss 2 will be small, if each pixel value is close to one of the pre-defined colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!gdown 1-07DCaJtJAlj1JdVMqtrob4d4Td7IuMw # Download white box model\n",
    "!git clone https://github.com/Gikaeh/Adversial-Physical-Attacks.git # Import necessary files\n",
    "!mv Adversial-Physical-Attacks/* .\n",
    "!rm -rf Adversial-Physical-Attacks\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox, Layout\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm.notebook import trange\n",
    "from extra_keras_datasets import stl10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load\n",
    "Uses Tensor Flow Keras library versus standalone Keras for better cohesion. Loads previously trained and finetuned VGG16 white box model for image recognition. This model was trained on 8000 images and tested on 5000 images from CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"test.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "Define Image dimensions and class list variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class list of stl10\n",
    "class_list = [\"airplane\", \"bird\", \"car\", \"cat\", \"deer\", \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"]\n",
    "\n",
    "# image sizes\n",
    "w, h, d = 96, 96, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting images with pred. class & confidence value\n",
    "Shows the images with the models predicted class label and confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, columns, rows, title, model, targets=None, gt=None, class_list= [\"airplane\", \"bird\", \"car\", \"cat\", \"deer\", \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"], w=96, h=96,d=3, size=(15, 8), ae=False):\n",
    "    fig=plt.figure(figsize=size)\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    columns = columns\n",
    "    rows = rows\n",
    "    for i in range(1, columns*rows +1):\n",
    "        img = images[i-1]\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        pred = model.predict(images[i-1].reshape(1, w, h, d))\n",
    "        pred_class = class_list[np.argmax(pred.flatten())]\n",
    "        pred_class_conf_prob = max(pred.flatten())\n",
    "        if targets is not None:\n",
    "            pred_target_class = max(pred.flatten()*targets[i-1])\n",
    "            plt.title(f\"{pred_class} ({pred_class_conf_prob:.2f})\\nTarget: {pred_target_class:.2f}\")\n",
    "        elif gt is not None:\n",
    "            plt.title(f\"{pred_class} ({pred_class_conf_prob:.2f})\\nG.T.: {gt[i-1]}\")\n",
    "        else:\n",
    "            plt.title(f\"{pred_class} ({pred_class_conf_prob:.2f})\")\n",
    "\n",
    "        if ae:\n",
    "            img = np.clip(img, 0, 1)\n",
    "\n",
    "        plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting images with mask over original\n",
    "Shows the original image with the designated mask over it. Also displaying the predicted label and confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_physical_adv(model, w, h, d, adv_images, target, save=True, name=\"test\", attack_type=\"targeted\", class_list=[\"airplane\", \"bird\", \"car\", \"cat\", \"deer\", \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"]):\n",
    "    # Ensure input is iterable if single example\n",
    "    if not isinstance(adv_images, (list, np.ndarray)):\n",
    "        adv_images = [adv_images]\n",
    "\n",
    "    for i, adv_image in enumerate(adv_images):\n",
    "        # Convert tensor to numpy array if needed\n",
    "        if tf.is_tensor(adv_image):\n",
    "            adv_np = adv_image.numpy()\n",
    "        else:\n",
    "            adv_np = adv_image\n",
    "\n",
    "        # Reshape to image dimensions\n",
    "        img_array = adv_np.reshape((h, w, d))\n",
    "\n",
    "        # Make prediction\n",
    "        pred = model.predict(np.expand_dims(img_array, 0)).flatten()\n",
    "        best = np.argmax(pred)\n",
    "        target_conf = pred[np.argmax(target.flatten())]\n",
    "\n",
    "        # Save image if requested\n",
    "        if save:\n",
    "            if not os.path.isdir(\"./generated_advs\"):\n",
    "                os.mkdir(\"./generated_advs\")\n",
    "            filename = f\"./generated_advs/{name}_{class_list[best]}_{pred[best]:.2f}.jpg\"\n",
    "            plt.imsave(filename, img_array)\n",
    "\n",
    "        # Create plot\n",
    "        plt.imshow(img_array)\n",
    "        if attack_type == \"targeted\":\n",
    "            plt.title(f\"Best: {class_list[best]} ({pred[best]:.2f})\\nTarget: {target_conf:.2f}\")\n",
    "        else:\n",
    "            plt.title(f\"Best: {class_list[best]} ({pred[best]:.2f}\\nG.T.: {target_conf:.2f})\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Evaluate white box model on set of nine images using the plot_images function mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load attack images\n",
    "org_images = []\n",
    "for path in os.listdir(\"./imgs_gui/\"):\n",
    "    img = plt.imread(\"./imgs_gui/\" + path)\n",
    "    org_images.append(img/255.)\n",
    "org_images = np.array(org_images) \n",
    "\n",
    "# Evaluate model\n",
    "plot_images(org_images, 3, 3, title=\"Original Images\", model=model, size=(10, 11))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Attack Class\n",
    "\n",
    "1. Initialize a variable $\\delta$ (noise) with random values and load an input image\n",
    "2. Multiply noise with a mask that only contains areas with ones and zeros. The ones in the mask indicate the changeable elements in $\\delta$: $mask * noise$ \n",
    "3. Compute the inverse of the mask and multiply it with the current image: $(1 - mask) * img$\n",
    "4. Add the masked image to the masked pertubation: $[mask * noise] + [(1 - mask) * img]$\n",
    "5. Feed the image to the classification network and get the prediction\n",
    "6. Calculate the loss\n",
    "7. Use Gradient Descent with the optimizer adam to change the elements of the noise to minimize loss\n",
    "8. Return to step 2 or stop the training if the number of epochs are reached\n",
    "\n",
    "These steps can be shown as:\n",
    "<figure style=\"text-align:center\">\n",
    "    <p></p>\n",
    "  <img src=\"charts/physical_process.png \" width=\"40%\">\n",
    "     <p></p>\n",
    "  <figcaption style=\"font-family:courier;\">Physical Attack Process</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     6,
     48,
     61,
     68,
     145
    ]
   },
   "outputs": [],
   "source": [
    "class PhysicalAttack():\n",
    "    def __init__(self, img_rows, img_cols, depth, n_classes, noisy_input_clip_min, noisy_input_clip_max, attack_lambda, print_lambda, lr, model, targeted):\n",
    "        # Input dimensions\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.depth = depth # Color channels\n",
    "        self.n_classes = n_classes # number of output classes\n",
    "\n",
    "        # Clipping bounds\n",
    "        self.noisy_input_clip_min = noisy_input_clip_min\n",
    "        self.noisy_input_clip_max = noisy_input_clip_max\n",
    "\n",
    "        # Loss weights\n",
    "        self.attack_lambda = attack_lambda # Weight for regularization loss\n",
    "        self.print_lambda = print_lambda # Weight for printable color loss\n",
    "\n",
    "        # Optimization\n",
    "        self.lr = lr\n",
    "        self.opt = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-08)\n",
    "        \n",
    "        # Model and mode\n",
    "        self.model = model\n",
    "        self.targeted = targeted # Targeted: Specific incorrect prediction; Untargeted: Any missclassification\n",
    "\n",
    "        # Noise variable (trainable)\n",
    "        self.noise = tf.Variable(\n",
    "            tf.random.uniform([img_rows, img_cols, depth], 0.0, 1.0), # Random noise with a min of 0 and max of 1\n",
    "            trainable=True, # Optimized noise during training\n",
    "            name='noise'\n",
    "        )\n",
    "\n",
    "        # Define input signatures for tf.function\n",
    "        self.train_step = tf.function(\n",
    "            self._train_step, # Compile training step during class initialization\n",
    "            input_signature=[\n",
    "                tf.TensorSpec(shape=(None, img_rows, img_cols, depth), dtype=tf.float32), # Input Image\n",
    "                tf.TensorSpec(shape=(None, n_classes), dtype=tf.float32), # Labels\n",
    "                tf.TensorSpec(shape=(img_rows, img_cols, depth), dtype=tf.float32), # Noise mask\n",
    "                tf.TensorSpec(shape=(None, img_rows, img_cols, depth), dtype=tf.float32), # Printable colors\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32), # Min clip\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32), # Max clip\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32), # Attack lamba\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32) # Print lambda\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _l1_norm(self, tensor):\n",
    "        # Computes L1 regularization loss\n",
    "        return tf.reduce_sum(tf.abs(tensor))\n",
    "\n",
    "    def _train_step(self, image_in, attack_target, noise_mask, printable_colors, noisy_input_clip_min, noisy_input_clip_max, attack_lambda, print_lambda):\n",
    "      with tf.GradientTape() as tape:\n",
    "          # Forward pass\n",
    "          noise_mul = tf.multiply(noise_mask, self.noise) # Apply mask to noise\n",
    "          noise_mul_clip = tf.clip_by_value(noise_mul, noisy_input_clip_min, noisy_input_clip_max) # Clip noise\n",
    "          inverse_masks = 1.0 - noise_mask\n",
    "          # Combine image and noise\n",
    "          noise_inputs = tf.clip_by_value(\n",
    "              tf.add(image_in * inverse_masks, noise_mul), # Add mask img to mask perturbation\n",
    "              noisy_input_clip_min, noisy_input_clip_max\n",
    "          )\n",
    "\n",
    "          # Model predictions\n",
    "          adv_pred = self.model(noise_inputs)\n",
    "\n",
    "          # Regularization loss\n",
    "          reg_loss = self._l1_norm(noise_mul)\n",
    "\n",
    "          # Classification loss\n",
    "          loss_f = tf.keras.losses.categorical_crossentropy(attack_target, adv_pred, from_logits=False)\n",
    "          loss = tf.cond(\n",
    "              tf.logical_not(self.targeted), # If untargeted, minimize confidence in true class\n",
    "              lambda: 1/(loss_f + 1e-3), # Inverse loss for untargeted 1/(CE+(1e-3))\n",
    "              lambda: loss_f # Direct loss for targeted\n",
    "          )\n",
    "\n",
    "          # Printable color loss\n",
    "          printab_diff = tf.math.squared_difference(noise_mul, printable_colors * noise_mask) #Difference from printable colors\n",
    "          printer_error = tf.reduce_sum(tf.reduce_prod(tf.reduce_sum(printab_diff, 3), 0))\n",
    "\n",
    "          # Total adversarial loss\n",
    "          adv_loss = loss + attack_lambda * reg_loss + print_lambda * printer_error\n",
    "\n",
    "      # Gradient calculation & application\n",
    "      grads = tape.gradient(adv_loss, [self.noise])\n",
    "      self.opt.apply_gradients(zip(grads, [self.noise]))\n",
    "\n",
    "      return [adv_loss, noise_inputs, loss, reg_loss, printer_error, noise_mul_clip, loss_f]\n",
    "\n",
    "    def generate(self, n_epochs, img, target, mask, print_colors, verbose=True):\n",
    "      # Initialize noise\n",
    "      self.noise.assign(tf.random.uniform(self.noise.shape, 0.0, 1.0))\n",
    "\n",
    "      for i in (trange(n_epochs) if verbose else range(n_epochs)):\n",
    "          outputs = self.train_step(\n",
    "              img, target, mask, print_colors,\n",
    "              self.noisy_input_clip_min, self.noisy_input_clip_max,\n",
    "              self.attack_lambda, self.print_lambda\n",
    "          )\n",
    "\n",
    "          if verbose and (i % 200 == 0):\n",
    "              adv_loss, _, loss_cat, _, loss_print, clipped_noise, _ = outputs\n",
    "              print(f\"\\nEpoch {i}: Total Loss: {float(adv_loss):.4f}, \"\n",
    "                    f\"Classification Loss: {float(loss_cat):.4f}, \"\n",
    "                    f\"Print Loss: {float(loss_print):.4f}\")\n",
    "              plt.imshow(clipped_noise.numpy().reshape(self.img_rows, self.img_cols, 3))\n",
    "              plt.show()\n",
    "\n",
    "      return outputs[1]  # Return noise_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gui Build with Ipywidget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "Contains Listener functions for the GUI display along with a read function for the images and a color load function for the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     7,
     18,
     32,
     43,
     75
    ]
   },
   "outputs": [],
   "source": [
    "def row(descr):\n",
    "    # This is used to create images and checkboxes next to each other\n",
    "    return widgets.Checkbox(description=descr, value=False, indent=False)\n",
    "\n",
    "\n",
    "def img_checkbox_listener(change):\n",
    "    # This method manually allows only one checkbox to be checked.\n",
    "    curr_descr = change[\"owner\"].description\n",
    "    for rb in rbs_imgs:\n",
    "        if rb.description != curr_descr:\n",
    "            rb.value=False\n",
    "            \n",
    "            \n",
    "def printer_loss_listener(change):\n",
    "    #This listener enables or disbables the printer hyperparameters if we don't want to include printable color\n",
    "    if change[\"new\"] == \"Enable\":\n",
    "        colors.disabled=False\n",
    "        print_lambda_slider.disabled = False\n",
    "    elif change[\"new\"] == \"Disable\":\n",
    "        colors.disabled=True\n",
    "        print_lambda_slider.disabled = True\n",
    "\n",
    "\n",
    "def attack_type_listener(change):\n",
    "    # This listener enables or disbables the target selection\n",
    "    if change[\"new\"] == \"untargeted\":\n",
    "        targets.disabled=True\n",
    "    elif change[\"new\"] == \"targeted\":\n",
    "        targets.disabled=False\n",
    "\n",
    "\n",
    "def _read_img(descr_list, is_mask, w, h, d):\n",
    "    # This method is used to read in attack images or masks    \n",
    "    pattern = re.compile(r\"'(.*?)'\")# Descriptions look like <img src='...'>. We extract the image path\n",
    "    \n",
    "    # Check if image is a mask\n",
    "    if is_mask:\n",
    "        masks = []\n",
    "        # Load all masks \n",
    "        for m in descr_list:\n",
    "            path = re.findall(pattern, m)[0]\n",
    "            mask = cv2.imread(path)\n",
    "            mask = mask/255.\n",
    "            masks.append(mask)\n",
    "        return masks\n",
    "    else:    \n",
    "        # Load attack image\n",
    "        path = re.findall(pattern, descr_list[0])[0]\n",
    "        img = plt.imread(path)\n",
    "        img = img/255.\n",
    "        img = img.reshape(1, w, h, d)\n",
    "        return img, path\n",
    "\n",
    "\n",
    "def _load_color_list(colors_str, w, h):\n",
    "    # Load the printable colors and convert them into images with w, h, d.\n",
    "    # For each color an image is generated that only contains pixel with that color\n",
    "    p = []\n",
    "    \n",
    "    # Iterate over each color in the string\n",
    "    for c in colors_str.split(\"\\n\"):\n",
    "        p.append(c.split(\",\"))       \n",
    "    \n",
    "    # Generate w, h, d images\n",
    "    p = map(lambda x: [[x for _ in range(w)] for __ in range(h)], p)\n",
    "    p = np.array(list(p)).astype(float)\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GUI Elements\n",
    "Builds the rows of the first 2 selection tabs that display the descriptions of the Original images and different mask for selection. It also creates all the hyperameter slider and selection options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     4,
     10,
     16,
     26,
     33,
     40,
     49,
     57,
     65,
     73,
     81,
     88,
     97,
     105
    ]
   },
   "outputs": [],
   "source": [
    "# We set 9 images fix from stl10 dataset\n",
    "rbs_imgs = (row(\"<img src='imgs_gui/airplane.jpg'>\"), row(\"<img src='imgs_gui/bird.jpg'>\"), \n",
    "            row(\"<img src='imgs_gui/car.jpg'>\"), row(\"<img src='imgs_gui/cat.jpg'>\"), \n",
    "            row(\"<img src='imgs_gui/deer.jpg'>\"), row(\"<img src='imgs_gui/dog.jpg'>\"),\n",
    "            row(\"<img src='imgs_gui/horse.jpg'>\"), row(\"<img src='imgs_gui/monkey.jpg'>\"),\n",
    "            row(\"<img src='imgs_gui/ship.jpg'>\"))\n",
    "# Creates horizontal rows of check boxs for selection of images  \n",
    "radio_buttons_imgs = widgets.HBox([*rbs_imgs], layout=Layout(height=\"300px\", width='100%', display='inline-flex',flex_flow='row wrap')) \n",
    "for rb in rbs_imgs:\n",
    "    rb.observe(img_checkbox_listener)\n",
    "\n",
    "    \n",
    "# Create checkboxes for the masks\n",
    "rbs = ( row(\"<img src='masks/mask1.png'>\"), row(\"<img src='masks/mask2.png'>\"), \n",
    "        row(\"<img src='masks/mask3.png'>\"), row(\"<img src='masks/mask4.png'>\"), \n",
    "        row(\"<img src='masks/mask5.png'>\"), row(\"<img src='masks/mask6.png'>\"))\n",
    "# Creates horizontal rows of check boxs for selection of masks\n",
    "radio_buttons = widgets.HBox([*rbs], layout=Layout(height=\"300px\", width='100%', display='inline-flex',flex_flow='row wrap'))\n",
    "\n",
    "\n",
    "# Create widgets for hyperparametrs\n",
    "# Width of image (currently fixed to stl10)\n",
    "w_input = widgets.Text(\n",
    "    value='96',\n",
    "    description='Input width:',\n",
    "    style ={'description_width': '150px'},\n",
    "    disabled=True\n",
    ")\n",
    "# Height of image (currently fixed to stl10)\n",
    "h_input = widgets.Text(\n",
    "    value='96',\n",
    "    description='Input height:',\n",
    "    style ={'description_width': '150px'},\n",
    "    disabled=True\n",
    ")\n",
    "# Depth of image (currently fixed to stl10)\n",
    "d_input = widgets.Text(\n",
    "    value='3',\n",
    "    description='Input depth:',\n",
    "    style ={'description_width': '150px'},\n",
    "    disabled=True\n",
    ")\n",
    "input_dims = HBox([w_input, h_input, d_input])\n",
    "\n",
    "# Number of classes in stl10\n",
    "n_classes = widgets.Text(\n",
    "    value='10',\n",
    "    description='Number of classes:',\n",
    "    style ={'description_width': '150px'},\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "# Number of epochs slider\n",
    "n_epochs = widgets.IntSlider(\n",
    "    value=1000,\n",
    "    min=100, max=2000, step=100,\n",
    "    description='Number Epochs:',\n",
    "    style ={'description_width': '150px', 'width':'100%'}\n",
    ")\n",
    "\n",
    "# Printer loss slider\n",
    "print_lambda_slider = widgets.FloatSlider(\n",
    "    value=0.01,\n",
    "    min=0, max=1, step=0.01,\n",
    "    description='Printer Lambda:',\n",
    "    style ={'description_width': '150px', 'width':'100%'}\n",
    ")\n",
    "\n",
    "# Learning rate slider\n",
    "lr = widgets.FloatSlider(\n",
    "    value=0.1,\n",
    "    min=0.01, max=0.1, step=0.01,\n",
    "    description='Learning Rate:',\n",
    "    style ={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "# Printable colors text area\n",
    "colors = widgets.Textarea(\n",
    "    value='0.01, 0.01, 0.01\\n1, 1, 1',\n",
    "    description='Color List:',\n",
    "    style ={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "# Attack type radio button\n",
    "attack_type = widgets.RadioButtons(\n",
    "    options=['targeted', 'untargeted'],\n",
    "    description='Attack type:',\n",
    "    disabled=False,\n",
    "    style ={'description_width': '150px'}\n",
    ")\n",
    "attack_type.observe(attack_type_listener)\n",
    "\n",
    "# Targets radio buttons\n",
    "targets = widgets.RadioButtons(\n",
    "    options=class_list,\n",
    "    description=\"Targets\",\n",
    "    disabled=False,\n",
    "    style ={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "# Print attack boolean radio button\n",
    "printer_attack = widgets.RadioButtons(\n",
    "    options=['Enable', 'Disable'],\n",
    "    description='Print Loss',\n",
    "    disabled=False,\n",
    "    style ={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "printer_attack.observe(printer_loss_listener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attack button listener\n",
    "This functions is the beginning of the attack for the GUI when the \"run attack button\" is pressed. It checks multiple conditional statements for image and mask selection and loads the white box model. The hyperparameters are taken from the GUI selection and passed to the instatiation of the Physical Attack class for noise generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def attack_button_listener(btn_object):\n",
    "    print(\"Starting the attack...\")\n",
    "\n",
    "    # Get attack image description\n",
    "    selected_imgs = []\n",
    "    for i, item in enumerate(rbs_imgs):\n",
    "        if item.value:\n",
    "            selected_imgs.append(item.description)\n",
    "\n",
    "    # User needs to select one image\n",
    "    if len(selected_imgs) == 0:\n",
    "        print(\"Please select at least one image\")\n",
    "    else:\n",
    "        # Read in image\n",
    "        img, path = _read_img(selected_imgs, False, int(w_input.value), int(h_input.value), int(d_input.value))\n",
    "\n",
    "        # Seperate true label from file path\n",
    "        path = path.split(\"/\")[1]\n",
    "        true_label = path.split(\".\")[0]\n",
    "\n",
    "        # Get masks descriptions\n",
    "        selected_masks = []\n",
    "        for i, item in enumerate(rbs):\n",
    "            if item.value:\n",
    "                selected_masks.append(item.description)\n",
    "\n",
    "        # User needs to select at least one mask\n",
    "        if len(selected_masks) == 0:\n",
    "            print(\"Please select at least one mask\")\n",
    "        else:\n",
    "            # Read in mask\n",
    "            masks = _read_img(selected_masks, True, int(w_input.value), int(h_input.value), int(d_input.value))\n",
    "\n",
    "            # Generate target vectors\n",
    "            target = np.zeros((1, len(class_list)))\n",
    "\n",
    "            # If targeted attack, then take the user defined target\n",
    "            if attack_type.value == \"targeted\":\n",
    "                target[0][class_list.index(targets.value)] = 1\n",
    "            # Else take the ground truth as target\n",
    "            else:\n",
    "                target[0][class_list.index(true_label)] = 1\n",
    "\n",
    "            # This is our white box model\n",
    "            model = tf.keras.models.load_model(\"test.keras\")\n",
    "\n",
    "            # Load the printable colors\n",
    "            p = _load_color_list(colors.value, int(w_input.value), int(h_input.value))\n",
    "\n",
    "            # check if we want to include colors inside the attack\n",
    "            print_lambda = 0\n",
    "            if printer_attack.value == \"Enable\":\n",
    "                print_lambda =  print_lambda_slider.value\n",
    "\n",
    "            # Instatiate the PhysicalAttack class and set parameter\n",
    "            attack = PhysicalAttack(img_rows=int(h_input.value),\n",
    "                                    img_cols=int(w_input.value),\n",
    "                                    depth=int(d_input.value),\n",
    "                                    n_classes=int(n_classes.value),\n",
    "                                    noisy_input_clip_min = 0,\n",
    "                                    noisy_input_clip_max = 1,\n",
    "                                    attack_lambda = 0, # Attack lambda is set to zero, because we don't care about how much we see the noise\n",
    "                                    print_lambda = float(print_lambda),\n",
    "                                    lr = float(lr.value),\n",
    "                                    model=model,\n",
    "                                    targeted = attack_type.value == \"targeted\")\n",
    "\n",
    "            # Container for generated images\n",
    "            adv_images = []\n",
    "            for i, m in enumerate(masks):\n",
    "                print(f\"Mask {i+1}\", \"#\"*20)\n",
    "                # Do the attack of n_epochs for each mask\n",
    "                adv_images.append(attack.generate(img=img,\n",
    "                                            n_epochs=int(n_epochs.value),\n",
    "                                            target=target,\n",
    "                                            mask=m,\n",
    "                                            print_colors=p,\n",
    "                                            verbose=1))\n",
    "            # Plot all generated images\n",
    "            plot_physical_adv(model, int(w_input.value), int(h_input.value), int(d_input.value),\n",
    "                              adv_images, target, save=True, name=true_label, attack_type=attack_type.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Run Attack\" button GUI element creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create attack button and listen to it with the above function\n",
    "button = widgets.Button(\n",
    "    description='Run Attack',\n",
    ")\n",
    "button.on_click(attack_button_listener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run attack with GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build gui\n",
    "tab_rb_imgs = VBox(children=[radio_buttons_imgs])\n",
    "tab_rb = VBox(children=[radio_buttons])\n",
    "tab_hp = VBox(children=[input_dims,\n",
    "                        \n",
    "                      attack_type,\n",
    "                        targets,\n",
    "                        n_epochs,\n",
    "                        lr,\n",
    "                        printer_attack,\n",
    "                        colors,\n",
    "                       print_lambda_slider\n",
    "                       \n",
    "                       ])\n",
    "\n",
    "gui = widgets.Tab(children=[ tab_rb_imgs, tab_rb, tab_hp])\n",
    "gui.set_title(0, 'Choose Image')\n",
    "gui.set_title(1, 'Choose Masks')\n",
    "gui.set_title(2, 'Hyperparameter')\n",
    "VBox(children=[gui, button])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different masks with STL10 test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load STL10 dataset\n",
    "Defines STL10 classes along with the pre-determined image dimensions. Splits the data between train and test sets in order train the different masks noise and test them on different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define stl10 class list and preprocess dataset\n",
    "class_list = [\"airplane\", \"bird\", \"car\", \"cat\", \"deer\", \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"]\n",
    "num_classes = 10\n",
    "\n",
    "# Define width, height and dimension of images\n",
    "w, h, d = 96, 96, 3\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_test, y_test), (x_train, y_train)= stl10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "# Stl10 datasets starts with class 1, not 0\n",
    "y_train = keras.utils.to_categorical(y_train-1, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test-1, num_classes)\n",
    "\n",
    "# Scale data between 0 and 1\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run attack on STL10 images\n",
    "The creation of adversarial images for numerous images with multiple masks will take a long time so the number_of_images should be changed to reflect amount of time to spend on this section. There is already a saved dump of 100 images (Uses 1st 3 masks) in the pickle_dumps folder for use.\n",
    "\n",
    "Can select the number of images to create the adversarial counterparts for and select what masks to use for adversarial counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Number of images included in the validation\n",
    "number_of_images = 101\n",
    "\n",
    "# Instatiate attack class\n",
    "attack = PhysicalAttack(img_rows=96,\n",
    "                        img_cols=96,\n",
    "                        depth=3,\n",
    "                        n_classes=10,\n",
    "                        noisy_input_clip_min = 0,\n",
    "                        noisy_input_clip_max = 1,\n",
    "                        attack_lambda = 0,\n",
    "                        print_lambda = 0,\n",
    "                        lr = 0.1,\n",
    "                        model=model,\n",
    "                        targeted = False)\n",
    "\n",
    "# Load the masks\n",
    "masks = []\n",
    "mask_paths = [\"masks/mask1.png\", \"masks/mask2.png\", \"masks/mask3.png\"]\n",
    "for m_path in mask_paths:    \n",
    "    mask = cv2.imread(m_path)\n",
    "    mask = mask/255\n",
    "    masks.append(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates and save a pickle file for later use. If a pickle dump is already made for the number images uses that instead. Also runs a evaluation on the base model for the loaded 2000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Attack the Images\n",
    "if not os.path.isfile(f\"pickle_dumps/adv_on_{number_of_images}.p\"):\n",
    "    if not os.path.isdir(f\"pickle_dumps\"):\n",
    "        os.mkdir(\"pickle_dumps\")\n",
    "    # Storage for all adv images\n",
    "    all_advs = []\n",
    "    # Dummy colors\n",
    "    with tf.device('/GPU:0'):\n",
    "        p = _load_color_list(\"1, 1, 1\", int(w_input.value), int(h_input.value)) \n",
    "        for i in trange(len(x_test[:number_of_images])):\n",
    "            t_image = x_test[i]\n",
    "            adv_images = []\n",
    "            for j, m in enumerate(masks):\n",
    "                # Do the attack of n_epochs for each mask\n",
    "                adv_images.append(attack.generate(img=t_image.reshape(1, 96, 96, 3), \n",
    "                                            n_epochs=400, \n",
    "                                            target=y_test[i:i+1], \n",
    "                                            mask=m, \n",
    "                                            print_colors=p, \n",
    "                                            verbose=0))\n",
    "            all_advs.append(adv_images)\n",
    "\n",
    "        # dump the results    \n",
    "        all_advs = np.array(all_advs)\n",
    "        pickle.dump( all_advs, open( f\"pickle_dumps/adv_on_{number_of_images}.p\", \"wb\" ))\n",
    "        print(f\"Dump new adversarial images\")\n",
    "else:\n",
    "    all_advs = pickle.load(open( f\"pickle_dumps/adv_on_{number_of_images}.p\", \"rb\" ))\n",
    "    print(\"Load existing dump of adversarial images\")\n",
    "\n",
    "# Evaluate base model on the 2000 test images     \n",
    "print(f\"Baseline of first {number_of_images} images: \", model.evaluate(x_test[:number_of_images], y_test[:number_of_images])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success rate of the mask\n",
    "Model predicts the class label for the adversarial images using the different mask and outputs the success rate as a percentage with higher being the percentage of the model being incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_advs.shape)\n",
    "\n",
    "# First extract images per mask and do reshape\n",
    "all_advs = np.array(all_advs)\n",
    "first_masks = all_advs[:, 0].reshape(-1, 96, 96, 3)\n",
    "second_masks = all_advs[:, 1].reshape(-1, 96, 96, 3)\n",
    "third_masks = all_advs[:, 2].reshape(-1, 96, 96, 3)\n",
    "\n",
    "# Check model acc, success rate is 1-model acc\n",
    "print(\"Succes rate of mask 1: \", 1- model.evaluate(first_masks, y_test[:number_of_images])[1])\n",
    "print(\"Succes rate of mask 2: \", 1- model.evaluate(second_masks, y_test[:number_of_images])[1])\n",
    "print(\"Succes rate of mask 3: \", 1- model.evaluate(third_masks, y_test[:number_of_images])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot examples\n",
    "Display 10 adversarial images of each masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_images(first_masks[20:30], 5, 2, title=\"Examples First Mask\", model=model, size=(10, 5))\n",
    "plot_images(second_masks[60:70], 5, 2, title=\"Examples Second Mask\", model=model, size=(10, 5))\n",
    "plot_images(third_masks[90:100], 5, 2, title=\"Examples Third Mask\", model=model, size=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical attacks on real world examples\n",
    "This section will take the noise created for specific images and replicate them as best as it can on real world versions of the images to see how the attacks compare. This is to test the possible changes in angles and lighting of the photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Original digital image\n",
    "img_org = plt.imread(\"imgs_adv/deer.jpg\")\n",
    "img_org = img_org/255.\n",
    "\n",
    "# Original attack image\n",
    "attack_org = plt.imread(\"imgs_adv/deer_airplane.jpg\")\n",
    "attack_org = attack_org/255.\n",
    "\n",
    "# Photo original image\n",
    "img_photo = plt.imread(\"imgs_adv/deer_org.jpg\")\n",
    "img_photo = img_photo/255.\n",
    "\n",
    "\n",
    "# Photo attack image\n",
    "attack_photo = plt.imread(\"imgs_adv/deer_adv.jpg\")\n",
    "attack_photo = attack_photo/255.\n",
    "\n",
    "\n",
    "plot_images(np.array([img_org, attack_org, img_photo, attack_photo]), columns=2, rows=2, model=model, title=\"Deer Example (Success)\", size=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Original digital image\n",
    "img_org = plt.imread(\"imgs_adv/airplane.jpg\")\n",
    "img_org = img_org/255.\n",
    "\n",
    "# Original attack image\n",
    "attack_org = plt.imread(\"imgs_adv/airplane_ship.jpg\")\n",
    "attack_org = attack_org/255.\n",
    "\n",
    "# Photo original image\n",
    "img_photo = plt.imread(\"imgs_adv/airplane_org.jpg\")\n",
    "img_photo = img_photo/255.\n",
    "\n",
    "\n",
    "# Photo attack image\n",
    "attack_photo = plt.imread(\"imgs_adv/airplane_adv_ship.jpg\")\n",
    "attack_photo = attack_photo/255.\n",
    "\n",
    "\n",
    "plot_images(np.array([img_org, attack_org, img_photo, attack_photo]), columns=2, rows=2, model=model, title=\"Airplane untargeted (Success)\", size=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Original digital image\n",
    "img_org = plt.imread(\"imgs_adv/airplane.jpg\")\n",
    "img_org = img_org/255.\n",
    "\n",
    "# Original attack image\n",
    "attack_org = plt.imread(\"imgs_adv/airplane_horse.jpg\")\n",
    "attack_org = attack_org/255.\n",
    "\n",
    "# Photo original image\n",
    "img_photo = plt.imread(\"imgs_adv/airplane_org.jpg\")\n",
    "img_photo = img_photo/255.\n",
    "\n",
    "\n",
    "# Photo attack image\n",
    "attack_photo = plt.imread(\"imgs_adv/airplane_adv_horse.jpg\")\n",
    "attack_photo = attack_photo/255.\n",
    "\n",
    "\n",
    "plot_images(np.array([img_org, attack_org, img_photo, attack_photo]), columns=2, rows=2, model=model, title=\"Airplane targeted (Failure)\", size=(15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Sources\n",
    "[1] [Robust Physical-World Attacks on Machine Learning Models](https://arxiv.org/abs/1707.08945)  \n",
    "[2] [Accessorize to a Crime: Real and Stealthy Attacks on\n",
    "State-of-the-Art Face Recognition](https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf)  \n",
    "[3] [Adam: A method for stochastic optimization](https://arxiv.org/abs/1412.6980)  \n",
    "[4] [Deep Learning, S.82.86](http://www.deeplearningbook.org)  \n",
    "[5] [GTRSB](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)  \n",
    "[6] [ImageNet-trained CNNs are biased towards texture](https://openreview.net/forum?id=Bygh9j09KX)  \n",
    "[7] [One Pixel Attack for Fooling\n",
    "Deep Neural Networks](https://arxiv.org/pdf/1710.08864.pdf)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
